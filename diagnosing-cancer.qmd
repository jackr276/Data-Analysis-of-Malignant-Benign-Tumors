---
title: "Lab 8: Diagnosing Cancer"
Instructor: Jack Robbins
Student: Redacted
---

```{r}
#| message: false
#| warning: false
#Instructor: Jack Robbins
#Student: Redacted

# load data and set "B" (benign) as the reference level
library(tidyverse)
cells <- read_csv("https://www.dropbox.com/s/0rbzonyrzramdgl/cells.csv?dl=1") %>%
  mutate(diagnosis = factor(diagnosis, levels = c("B", "M")))
library(ggplot2)
```

The diagnosis is in the column named `diagnosis`; each other column should be used to predict the diagnosis.

### Understanding and Exploring the Data

#### Question 1

What is the unit of observation in this data frame?

Benign/malignant


#### Question 2

Use a box plot to compare the `radius_mean` for benign vs. malignant biopsies. What is the takeaway from this plot?
```{r}

boxplot(radius_mean ~ diagnosis,
        data = cells,
        horizontal = TRUE,
        main = "Mean Radius By Diagnosis",
        xlab = "Mean Radius",
        ylab = "Diagnosis")

```
Based on this boxplot, malignant tumors have a larger mean radius. We will keep this
in mind when doing our regression later.


#### Question 3

Select another variable that you suspect might be a good predictor of the diagnosis. Build a plot to illustrate their association and provide an interpretation of what the plot shows.
```{r}
boxplot(texture_mean ~ diagnosis,
        data = cells,
        horizontal = TRUE,
        main = "Mean Texture By Diagnosis",
        xlab = "Mean Texture",
        ylab = "Diagnosis")
```
The mean texture is higher for malignant tumors than benign ones. Benign have many
more outliers than malignant ones.


#### Question 4

Make a plot that examines the association between two predictors, `radius_mean` and `area_mean`, and calculate the pearson correlation coefficient between these them. How would you describe the strength and shape of their association? What might cause this shape?
```{r}
cells %>%
  ggplot(aes(x=radius_mean, y=area_mean)) +
  geom_point()+
  xlab("Mean Radius") +
  ylab("Mean Area") +
  ggtitle("Mean Area VS. Mean Radius")

cor(cells$radius_mean, cells$area_mean)
```
The Mean Radius and Mean Area are very strongly linearly correlated, with a correlation
coefficient of 0.98. This happens because, usually, for symmetrical tumors, the area and
radius are directly related.


#### Question 5

Make a single plot that examines the association between `radius_mean` and `radius_sd` separately for each diagnosis (hint: `aes()` should have three arguments). Calculate the correlation between these two variables for each diagnosis.


    Give an interpretation of these results. In particular comment on

    - Is the relationship between `radius_mean` and `radius_sd` different for benign biopsies vs. malignant biopsies?

    - If so, can you give an explanation for this difference?
```{r}
cells %>%
  ggplot(aes(x=radius_mean, y=radius_sd, color = diagnosis)) +
  geom_point() +
  xlab("Radius Mean") +
  ylab("Radius Standard Deviation")+
  ggtitle("Standard deviation vs. Mean of Radius")

benign <- filter(cells, diagnosis == "B")
malignant <- filter(cells, diagnosis == "M")

cor(benign$radius_mean, benign$radius_sd)

cor(malignant$radius_mean, malignant$radius_sd)
```
Benign tumors have a much lower standard deviation than malignant. As we can
see via the correlation coefficients, there is a difference for malignant vs. 
benign. For benign tumors, the correlation coefficient of -0.03 suggests that
there is no relationship between the radius mean and radius standard deviation
for benign tumors. On the other hand, the correlation coefficient of 0.63 for
malignant tumors, suggests that there is a moderate, positive, linear correlation
between mean radius and radius standard deviation for malignant tumors. Malignant
tumors are more irregularly shaped, and therefore would have a higher radius standard
deviation.


### Diagnosing Biopsies

#### Question 6

Split the full cells data set into a roughly 80-20 train-test set split. How many observations do you have in each?
```{r}
set.seed(5012023)

set_type <- sample(x=c("train", "test"),
                    size = 569,
                    replace = TRUE,
                    prob = c(0.79, 0.21))

cells <- cells %>%
                mutate(set_type = set_type)

cells_train <- cells %>% filter(set_type == 'train')
cells_test <- cells %>% filter(set_type == 'test')
dim(cells_train)
dim(cells_test)
```

#### Question 7

Using the training data, fit a simple logistic regression model that predicts the diagnosis using the mean of the texture index using a threshold of .5. What would your model predict for a biopsy with a mean texture of 15? What probability does it assign to that outcome?
```{r}
# are we going to have to adjust texture mean logarithmically?
p_texture_mean <- ggplot(cells, aes(x=compactness_mean, fill=diagnosis))+
                geom_density(alpha=0.4)
p_texture_mean


model1 <- glm(diagnosis ~ texture_mean, data = cells_train, family = "binomial")
model1

p_hat_model1 <- predict(model1, cells_train, type = 'response')

prediction <- tibble(y=cells_train$diagnosis) %>%
              mutate(p_hat_model1 = p_hat_model1,
                     y_hat_model1 = p_hat_model1 > 0.5,
                     misclassified = (y == "M" & y_hat_model1 == FALSE) | 
                       (y == "B" & y_hat_model1 == TRUE) )

prediction


newdata = data.frame(texture_mean = 15)
predict(model1, newdata, type='response')
```
The model says that there is an 18% chance it is malignant. Therefore it classified the observation as
benign.


#### Question 8

Calculate the misclassification rate first on the training data and then on the testing data. Is there any evidence that this model is overfitting? How can you tell one way or the other?
```{r}
# Calculating misclassification for the training data
num_miss <- sum(prediction$misclassified, na.rm = TRUE )
dim(cells_train)
100*(num_miss / 451) 

###############################################################################

p_hat_model2 <- predict(model1, cells_test, type = 'response')

prediction2 <- tibble(y=cells_test$diagnosis) %>%
              mutate(p_hat_model2 = p_hat_model2,
                     y_hat_model2 = p_hat_model2 > 0.5,
                     misclassified = (y == "M" & y_hat_model2 == FALSE) | 
                       (y == "B" & y_hat_model2 == TRUE))

num_miss2 <- sum(prediction2$misclassified, na.rm = TRUE )
dim(cells_test)
100*(num_miss2 / 118) 


```
Training Data: 27.5% Misclassification Rate
Testing Data: 35.5% Misclassification Rate

There is some evidence that this model is overfitting. We can tell because the 
misclassification rate for the training data is a few percentage points lower
than that of the testing data.


#### Question 9

Build a more complex model to predict the diagnosis using five predictors of your choosing, then calculate the testing and training misclassification rate. Is there evidence that your model is overfitting? How can you tell one way or the other?
```{r}
model2 <- glm(diagnosis ~ texture_mean + radius_mean + radius_sd + perimeter_mean
              + compactness_mean, data = cells_train, family = "binomial")
model2

p_hat_model3 <- predict(model2, cells_train, type = 'response')

prediction3 <- tibble(y=cells_train$diagnosis) %>%
              mutate(p_hat_model3 = p_hat_model3,
                     y_hat_model3 = p_hat_model3 > 0.5,
                     misclassified = (y == "M" & y_hat_model3 == FALSE) | 
                       (y == "B" & y_hat_model3 == TRUE) )
prediction3


# Calculating misclassification for the training data
num_miss3 <- sum(prediction3$misclassified, na.rm = TRUE )
dim(cells_train)
100*(num_miss3 / 451) 

###############################################################################

p_hat_model4 <- predict(model2, cells_test, type = 'response')

prediction4 <- tibble(y=cells_test$diagnosis) %>%
              mutate(p_hat_model4 = p_hat_model4,
                     y_hat_model4 = p_hat_model4 > 0.5,
                     misclassified = (y == "M" & y_hat_model4 == FALSE) | 
                       (y == "B" & y_hat_model4 == TRUE) )

num_miss4 <- sum(prediction4$misclassified, na.rm = TRUE )
dim(cells_test)
100*(num_miss4 / 118) 


```
The disparity in the misclassification rates still shows some evidence of overfitting,
but it less than our original model. 

#### Question 10

If you were to deploy your method in a clinical setting to help diagnose cancer, which type of classification error would be worse and why?

It would be worse if the model classified a tumor as benign while the tumor was actually
malignant(false negative). This would be worse because the patient would subsequently not get the 
appropriate treatment for a malignant tumor. 


#### Question 11

Calculate the total number of false negatives in the test data set when using your simple model with only one variable.
```{r}
prediction2
# looking for y = M but y_hat_model1 = False or p_hat_model1 <= 0.5
malignant_pred <- prediction2 %>% filter(y == "M")
sum(malignant_pred$misclassified, na.rm = TRUE)
```
There are 17 false negatives when the model predicts based on the testing 
predictions.

#### Question 12

What can you change about your classification rule to lower the number of false negatives? Make this change and calculate the new number of false negatives.

We can try lowering our thresh-hold to 0.35. This will lower the number of false negatives. 
```{r}
p_hat_model2 <- predict(model1, cells_test, type = 'response')

prediction2 <- tibble(y=cells_test$diagnosis) %>%
              mutate(p_hat_model2 = p_hat_model2,
                     y_hat_model2 = p_hat_model2 > 0.35,
                     misclassified = (y == "M" & y_hat_model2 == FALSE) | 
                       (y == "B" & y_hat_model2 == TRUE))
prediction2
malignant_pred2 <- prediction2 %>% filter(y == "M")
malignant_pred2
sum(malignant_pred2$misclassified, na.rm = TRUE)
```

#### Question 13

Calculate the testing misclassification rate using your new classification rule. Did it go up or down? Why?
```{r}
num_miss5 <- sum(prediction2$misclassified, na.rm = TRUE )
dim(cells_test)
100*(num_miss5 / 118) 
```
It went down. This could be the case because it could be much more likely to have a false negative as
opposed to a false positive. So, lowering the number of false negatives could lower the 
misclassification rate overall.






